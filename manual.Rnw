
\documentclass[12pt]{article}


<<eval=FALSE,echo=FALSE>>=
##Make sure these packages are installed before trying to compile
install.packages(c("ggplot2", "gridExtra","RColorBrewer","knitr","beeswarm","UsingR","tidyr","rmarkdown","clinfun","RVAideMemoire"))
source("http://www.bioconductor.org/biocLite.R")
biocLite("BiocStyle")
@



<<knitr, echo=FALSE, results="hide">>=
library("knitr")
opts_chunk$set(tidy=FALSE,dev="pdf",fig.show="as.is",
               fig.width=10,fig.height=6,
               message=FALSE,eval=TRUE,warning=FALSE,echo=TRUE)
@ 

<<style, eval=TRUE, echo=F, results="asis">>=
BiocStyle::latex()
@
\usepackage{ifthen} 
\usepackage{xcolor,colortbl}
\newboolean{includethis} 
\setboolean{includethis}{true}

\newcommand{\ifinclude}[1]{\ifthenelse{\boolean{includethis}}{#1}{}} 



\title{Further Statistical Analysis using R}
\author{Mark Dunning, Matt Eldridge and Sarah Vowler \thanks{Acknowledgements: Sarah Dawson}}
\date{Last Document revision: \today}
\begin{document}


\maketitle
\tableofcontents

\section{Course Introduction}

\centerline{\includegraphics[width=4cm,height=4cm]{images/fisher.jpg}}

\textit{"To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of."}. R.A. Fisher, 1938\\


The goals of statistical methods could be summarised as follows:
\begin{itemize}
\item{drawing conclusions about a population by analysing data on just a sample;}
\item{evaluating the uncertainty in these conclusions; and,}
\item{designing the sampling approach so that valid and accurate conclusions can be made from the data collected.}
\end{itemize}

The statistical approach used is dependent on the data type. In this document we will describe methods for comparing multiple groups (\textbf{ANOVA} (analysis of variance), and \textbf{non-parametric} alternatives), and \textbf{Linear Regression}. We will assume you are already familiar with methods to perform one-sample or two-sample tests, as described in our \href{http://bioinformatics-core-shared-training.github.io/IntroductionToStats/manual.pdf}{\color{blue}{Introductory Statistics}} course. After describing the assumptions of each test, we will give a worked example in R.

\subsection{Exploratory Analysis}
Before conducting a formal analysis of our data, it is always a good idea to run some exploratory checks of the data: 
\begin{itemize}
\item{To check that the data has been read in or entered correctly; }
\item{To identify any outlying values and if there is reason to question their validity, exclude them or investigate them further; }
\item{To see the distribution of the observations and whether the planned analyses are appropriate.}
\end{itemize}
It's always a good idea to calculate some summary statistics for your data, such as the mean and standard deviation, or the median and inter-quartile range if your data is skewed. You should also consider whether there may be outliers in your data (but do not remove them from the analysis without good reason) or whether there may be missing data. Summary statistics were covered in detail in our \href{http://bioinformatics-core-shared-training.github.io/IntroductionToStats/manual.pdf}{\color{blue}{Introductory Statistics}} course.

\subsection{Statistical Tests - basic setup}

There are four key steps in every statistical test:
\begin{itemize}
\item{1. Formulate a \textbf{null hypothesis}, H$_0$. This is the working hypothesis that we wish to disprove.}
\item{2. Under the assumption that the \textbf{null hypothesis} is true, calculate a \textbf{test statistic} from the data.}
\item{3. Determine whether the \textbf{test statistic} is more extreme than we would expect under the \textbf{null hypothesis}, i.e. look at the \textbf{$p$-value}.}
\item{4. Reject or do not reject the \textbf{null hypothesis}.}
\end{itemize}
As the name suggests, the null hypothesis typically corresponds to a \textbf{null} effect. 

For example, there is \textbf{no difference} in the measurements in group 1 compared with group 2. A small $p$-value indicates that the probability of observing such a test statistic as small under the assumption that the null hypothesis is true. If the $p$-value is below a pre-specified \textbf{significance level}, then this is a \textbf{significant result} and, we would conclude, there is evidence to reject the null hypothesis.

The \textbf{significance level} is most commonly set at 5\% and may also be thought of as the \textbf{false positive rate}. That is, there is a 5\% chance that the null hypothesis is true for data-sets with test statistics corresponding to $p$-values of less than 0.05 – i.e. we may wrongly reject the null hypothesis when the null hypothesis is true (false positive).

<<echo=FALSE,fig.show='asis',fig.height=5,fig.width=10,warning=FALSE,message=FALSE>>=

library(ggplot2)
x     = rnorm(10000, 0, 1)
sd <- sd(x)
me <- mean(x)

dat <- data.frame(x=x)
rects <- data.frame(xstart = c(-1.96,1.96), xend = c(-Inf,Inf),col=c("A","A"))

normdist <- ggplot() +   geom_histogram(data = dat, aes(x))+  geom_rect(data = rects, aes(xmin = xstart, xmax = xend, ymin = -Inf, ymax = Inf),alpha=0.4,fill="yellow") + ylab("") + xlab("") + theme(legend.position="none")
            
normdist
@

Equally, we may make \textbf{false negative} conclusions from statistical tests. In other words, we may not reject the null hypothesis when the null hypothesis is, in fact, not true. However, statisticians tend not to talk about false negative rates, preferring instead to refer to \textbf{power}, which is 1-false negative rate. 

The \textbf{power} of a statistical test will depend on:

\begin{itemize}
\item{The \textbf{significance level} - a 5\% test of significance will have a greater chance of rejecting the null than a 1\% test because the strength of evidence required for rejection is less.}
\item{The \textbf{sample size} - the larger the sample size, the more accurate our estimates (e.g. of the mean) therefore, we can differentiate between the null and alternative hypotheses more clearly.}
\item{The \textbf{size of the difference or effect} we wish to detect - bigger differences (i.e. alternative hypotheses) are easier to detect than smaller differences.}
\item{The \textbf{variability}, or standard deviation, of the observations - the more variable our observations, the less accurate our estimates therefore, it is more difficult to differentiate between the null and alternative hypotheses.}
\end{itemize}


\begin{table}[h]

\centering

\begin{tabular}{| l | l | l|}
   \hline
  & \textbf{Null hypothesis does not hold} &\textbf{Null hypothesis holds} \\
  \hline
  \textbf{Reject null hypothesis} & {\cellcolor{Orchid}Correct \textit{True Positive}} & {\cellcolor{CadetBlue}Wrong \textit{False positive}} \\
  \textbf{Do no reject null hypothesis} & {\cellcolor{CadetBlue}Wrong \textit{False negative}} & {\cellcolor{Orchid}Correct \textit{True negative}} \\
  \hline
\end{tabular}
\caption{Error definitions}
\label{errors}
\end{table}

\section{R Introduction}

To install R visit \href{www.r-project.org}{\color{blue}{www.r-project.org}}. In the 'Getting Started' box half-way down the page follow the 'download R' link. On the next page choose the appropriate operating system for your computer from the three 'Download R for...' options. 

Following this link will start the installation of R. If you get a security warning select 'Run'. Follow the directions in the install wizard to install R. We haven chosen to run {\tt R} through the popular \href{https://www.rstudio.com/products/RStudio/#Desktop}{\color{blue}{Rstudio}} interface, which you will also need to install. The version of R used to write this manual is {\color{red}{\Sexpr{paste(R.Version()$major, R.Version()$minor,sep=".")}}}. Please bear in mind that the version number you download may be different as new versions are released yearly. 


This manual, and the accompanying practical will assume some famililarity with the R statistical language. In particular, you should be familiar with the following concepts:

\begin{itemize}
\item{Using the RStudio program}
\item{Setting your working directory}
\item{Creating variables and basic object types; in particular vectors and data frames}
\item{Using built-in R functions}
\item{Using R to get help on functions}
\item{Subset operations for vectors and data frames using the [] notation}
\item{Reading tabular data into R}
\item{Basic plots; scatter plots, boxplot and histogram}
\end{itemize}

Several Online videos are available that cover this materials. For example

\begin{itemize}
\item{\href{http://shop.oreilly.com/product/0636920034834.do}{\color{blue}{http://shop.oreilly.com/product/0636920034834.do}}}
\item{\href{http://blog.revolutionanalytics.com/2012/12/coursera-videos.html}{\color{blue}{http://blog.revolutionanalytics.com/2012/12/coursera-videos.html}}}
\item{\href{http://bitesizebio.com/webinar/20600/beginners-introduction-to-r-statistical-software}{\color{blue}{http://bitesizebio.com/webinar/20600/beginners-introduction-to-r-statistical-software}}}
\end{itemize}

\subsection{R packages Used}
\label{installPackages}
In order to run the examples in this manual, and the practical, you will need to execute the following command in R to install the required packages. Or if you prefer, packages can be installed via the \textbf{Packages} tab in RStudio.

<<eval=FALSE>>=
install.packages(c("tidyr","beeswarm","RColorBrewer","clinfun","RVAideMemoire"))
@

You will not need to type this command more than once, so long as you keep the same version of R. Once these packages have been installed, you will need to load each of libraries into R when you require them. e.g.

<<eval=FALSE>>=
library(tidyr)
@

Alternatively, packages can be loaded from the "Packages" tab in the lower-right panel of RStudio.
\subsection{Formatting data for Statistical Testing in R}
\label{dataManip}
Before conducting a statistical test in R we often need to manipulate our data into a particular format. If you are familiar with working with numerical data in Excel (or similar), then you probably represent your data in tabular format. See Table \ref{messy.eg} for an example taken from the seminal 'Tidy Data' paper by Hadley Wickham \cite{tidyr:paper}.

<<echo=FALSE,results='asis'>>=
library(xtable)
messy <- data.frame(Name = c("John Smith","Jane Doe","Mary Johnson"),treatmenta = c("-",4,6),treatmentb=c(18,1,7))
print(xtable(messy,label="messy.eg",caption="An imaginary dataset in human-readable format."))
@

Table \ref{messy.eg} is easy for humans to comprehend; we can easily scan the table and identify that John Smith has a measurement of 18 for 'treatmenta' and no observation for 'treatmentb'. However, it is not very efficient for computers to deal with data in this format. For the majority of statistical testing that is performed in R, we think of the observations in our datasets as being explained by a series of factors, or explanatory variables. For example, in our imaginary dataset we have a measurement for each patient that belongs to either treatment group a, or treatment group b. Thus, the factor in this case would be treatment. Table \ref{tidy.eg} shows the same dataset, but in a \textit{'tidy'} format with \textit{variables} as columns and each row forming a different \textit{observation}. Sometimes this is also referred to as \textit{long} data, as opposed to the \textit{wide} format of the original table.

<<echo=FALSE,results='asis'>>=

tidy <- data.frame(name = c("Jane Doe","Jane Doe", "John Smith", "John Smith", "Mary Johnson","Mary Johnson"), treatment = rep(c("a","b"),3), n = c(4,1,NA,18,6,7))

print(xtable(tidy,label="tidy.eg",caption="The same imaginary dataset, but in tidy format."))

@

Several options exist in R to transform our data in this manner; such as the \Rfunction{stack} function from base R, to the \CRANpkg{reshape} and \CRANpkg{reshape2} packages. However, we will describe the \CRANpkg{tidyr} package, which is the evolution of \CRANpkg{reshape} and \CRANpkg{reshape2} and is nicely integrated with other advanced manipulation tools such as \CRANpkg{dplyr} and \CRANpkg{ggplot2}. 

We will now illustrate the how to transform our imaginary dataset into a suitable format for statistical analysis using \CRANpkg{tidyr}. The goal of such an operation is to take the \Rcode{treatmenta} and \Rcode{treatmentb} columns and create two separate columns; one which indicates whether a particular observation was made for treatment a or treatmentb, and the corresponding values. In the language of \CRANpkg{tidyr}, we say the the treatment variable is a \textit{key}.

<<>>=
messy <- data.frame(Name = c("John Smith","Jane Doe","Mary Johnson"),
                    treatmenta = c(NA,4,6),treatmentb=c(18,1,7))
@

The function to be used is called \Rfunction{gather} \footnote{Don't forget that you can get help on this function using \Rcode{?gather}}. The function is sometimes able to take a data frame as input and 'guess' what the tidy form of that dataset should look like. Finer control is possible, such as specifying column names in the output. In this example, we specify that we want a column in the output called \Rcode{'treatment'} for our new variable (the key), and \Rcode{'n'} for the corresponding values. The final argument is for specifying which columns in the input data are to be decomposed.

<<>>=
library(tidyr)
tidy <- gather(messy,treatment,n,treatmenta,treatmentb)
tidy
@

Note that we can use shortcuts to select columns, such as selecting all columns in the range \Rcode{treatmenta} to \Rcode{treatmentb}, or all columns except \Rcode{Name}. You should see that these give the same result. 
<<eval=FALSE>>=
gather(messy,treatment,n,treatmenta:treatmentb)
gather(messy,treatment,n,-Name)

@

With our data in this format, we can now specifiy various formulas using the tilde ($\sim$) syntax. You may have already seen the following syntax which will plot a boxplot with explanatory variable \Rcode{treatment} on the x-axis, and observations \Rcode{n} on the y-axis.

<<>>=
boxplot(tidy$n~tidy$treatment)
@

As we often have the variable and values that we wish to plot in the same data frame, there are a couple of shortcuts to avoid using the \$ syntax (which is a bit in-elegant). Firstly, functions such as \Rfunction{boxplot} and other statistical testing functions (e.g. \Rfunction{t.test}) have a \Rcode{data} argument. This allows the name of a data frame to be specified and the variables can be referred to by name.

<<eval=FALSE>>=
boxplot(n~treatment,data=tidy)
@

The \Rfunction{with} function also performs the same task;

<<eval=FALSE>>=
with(tidy, boxplot(n~treatment))
@


\section{Comparing Multiple Groups}

ANOVA stands for \textit{an}alysis \textit{o}f \textit{v}ariance. There are three main types of ANOVA: one-way, two-way and repeated measures. In this course our main focus will be on the one-way ANOVA.

\subsection{One-way ANOVA}
\label{oneWayAnova}
The two-sample t-test is useful when we have just two groups of continuous data to compare. When we want to compare more than two groups, a one-way ANOVA can be used to simultaneously compare all groups, rather than carrying out several individual two-sample t-tests.  The main advantage of doing this is that it reduces the number of tests being carried out, meaning that the type I error rate (the probability of seeing a significant result just by chance) does not become inflated. 

A one-way ANOVA compares group means by partitioning the variation in the data into \textbf{between group} variance and \textbf{within group variance} (see Table \ref{anova-table}). 

\begin{table}[h]

\centering

\begin{tabular}{| l | l | l|l|l|}
\hline
\textbf{Source of Variation} & \textbf{Degrees of Freedom} & \textbf{Sum of Squares} & \textbf{Mean Squares} & $F_{k-1,N-k}$ \\
\hline
Between groups & $k - 1$ &  $BSS = \sum_{k}^{i=1} n_iy^2_i - \frac{T^2}{N}$ & $S^2_B = \frac{BSS}{k -1}$ & \\
\hline
Within groups & $N - k$ & $WSS = S - \sum_{i=1}^{k} n_iy^2_i $ & $S^2_W = \frac{WSS}{N-k}$ & \\
\hline
Total & $N - 1 $ & TSS = BSS + WSS & & $\frac{S^2_B}{S^2_W}$\\
\hline
\end{tabular}
\caption{One-way ANOVA table}
\label{anova-table}
\end{table}

The between group variance is divided by the within group variance to give an F statistic. This tells us the ratio of between group variation to within group variation. A large F-value implies that there are significant differences between groups and conversely a small F-value implies there are not significant differences between groups. Giving this a little bit of thought, the idea becomes more intuitive:

\begin{itemize}
\item{If the variance within groups is small, but between groups the variance is very large, we can infer that there are likely to be differences between groups. Following this theory, the F statistic is calculated by dividing the between group variation by the within group variation. So in this scenario, we divide a large number by a comparatively small number and this leaves us with a large(ish) number for our F statistic, corresponding to a small $p$-value.}
\item{If the variance within groups is large, but between groups the variance is also large, it is more difficult to know whether the groups truly differ in their mean value. In this scenario, the F statistic is calculated by dividing a large number by another large number - depending on the relative size of these two large numbers we may be left with a large or small number for our F statistic. The same is true when the variance within and between groups are both small.}
\item{Finally, if the variance within group is large, but between groups the variance is very small, we can infer that there are unlikely to be differences between groups. In this scenario, the F statistic is calculated by dividing a small number by a large number and this will result in a small(ish) number for our F statistic, corresponding to a large $p$-value.}
\end{itemize}
Luckily, you won't need to do the calculations from Table \ref{anova-table} by hand as R will do these all for you, but it's good to have an appreciation of what the test is actually doing in the background.

Obviously the outcome of the one-way ANOVA depends on the data available. If there is high variation in the data, a much larger sample size will be needed to detect a difference between groups. Likewise, if we are interested in detecting very small differences between groups a larger sample size will also be required. 

\subsubsection{ANOVA assumptions}
\label{anova-assumptions}
There are several assumptions behind the one-way ANOVA:

\begin{itemize}
\item{Random selection of observations}
\item{Normally distributed response (assessed for each group separately)}
\item{Approximately equal variance across the groups}
\item{Independent observations}
\end{itemize}
Like most statistical tests, ANOVA assumes that there is a random selection of observations from the population of interest. If this is not the case then any results obtained from the test on the sample of observations may not generalise to the population as a whole. 

The main assumption of ANOVA is that the distribution of the response variable should be normally distributed for each group being compared. This can be assessed prior to fitting the ANOVA by constructing a histogram of the response variable for each group being compared. This can also be assessed after fitting the ANOVA by constructing a normal probability plot of the residuals (sometimes called a QQ-plot).

Another important assumption is that there is approximately equal variance across the groups being compared. This assumption is important because of the way the F-test in the ANOVA uses the pooled variance across groups. If one group has a much larger variance than another group, the results of the F-test may not be valid. The equal variance assumption can be assessed using either Bartlett's or Levene's test (REMEMBER! this adds to the multiple testing problem), or visually using a histogram plotted separately for each group. 

The final assumption of the one-way ANOVA is the independence of observations. There is no easy way of assessing independence, so a lot of people overlook this assumption. However, a little thought about where the data comes from and how it was collected can give us a good indication of whether the observations are independent or not. Things like taking observations from related individuals or having multiple measurements per subject will cause the independence assumption to be invalid. You should ask the question "is there any reason why any of the measurements are more likely to be similar than any others?" If the answer is no then it is safe to assume independence of observations.

When the F-test provides a significant result, it tells us that there is at least one difference in the groups. However, it does not tell us group is different. We may be interested in making comparisons between pairs of groups to identify where the difference lies and estimate the size of the difference (the effect size). This can be done by using unpaired two-sample t-tests. If we wish to make multiple comparisons we must be careful to adjust for multiple testing. R provides several different types of multiple-testing adjustment, each suiting different types of comparisons. These are discussed in more detail in the section \ref{choosing-the-correct-post-hoc-test}.  

\subsubsection{Choosing the correct post-hoc test}
\label{choosing-the-correct-post-hoc-test}

\begin{table}[h]

\centering

\begin{tabular}{| l | l|}
\hline
Tukey & Compare all pairs of columns \\
\hline
Bonferroni & Compare all pairs of columns OR compare selected pairs of columns \\
\hline
Dunnett & Compare all columns vs. control column \\
\hline
Trend test & Test for linear trend in means across columns \\
\hline 
\end{tabular}
\caption{Multiple-testing adjustment methods}
\label{post-test}
\end{table}

\subsubsection{One-way ANOVA Example}

The protein expression level was measured in 5 cell types from a single cell line. We want to know whether there are any differences in the expression level between the five different cell types. The raw data are given in Table \ref{protdata}. These data come from the Babraham Bioinformatics course \href{http://www.bioinformatics.babraham.ac.uk/training.html#prism}{\color{blue}{Statistical Analysis using GraphPad Prism}}.


<<echo=FALSE,results='asis'>>=
protdata <- read.csv("protein-expression.csv")
library(xtable)
xtable(protdata,label = "protdata",caption = "Protein Expression data")
@

Our \textbf{null hypothesis} is that the mean value is the same in each of the five groups.

Our \textbf{alternative hypothesis} is that the mean value is different in one or more of the five groups.

These data can be read using the \Rfunction{read.csv} function in R, which will create a \textit{data frame} representation. 
<<>>=
proteinData <- read.csv("protein-expression.csv")
@

At this point, it is a good idea to inspect the data to make sure they have been imported correctly. Sometimes R will read data without complaint, but create an object that you can't actually use for analysis. If you are using RStudio, the command \Rcode{View(proteinData)} will bring-up a display of the dataset. Otherwise the following commands will tell you about the dimensions of the data, first few lines and numerical summary of each column.

<<>>=
head(proteinData)
dim(proteinData)
summary(proteinData)
@

\subsubsection{Checking the model assumptions}

A boxplot of these data can be created using the \Rfunction{boxplot} function, which allows us to compare the median and inter-quartile range (IQR) of each cell type. Optionally, we can use the \CRANpkg{beeswarm} package to overlay individual points on the plot. See Figure \ref{fig:boxplot}

<<boxplot, fig.lp="fig:",fig.cap="Boxplot of the protein expression levels for five cell types">>=
library(beeswarm)
library(RColorBrewer)
boxplot(proteinData,xlab="Cell Type",ylab="Protein Expression",main="Protein Expression")
beeswarm(proteinData, add=TRUE,pch=16,col=brewer.pal(5,"Set1"),method="swarm")
@

\bioccomment{The \CRANpkg{RColorBrewer} package is also used to define a colour palette for the dataset.}

Figure \ref{fig:boxplot} shows us that the median value varies between the five groups. We can also see that the data is skewed for cell types A and D, as the bar in the middle, which shows the median, is not equally between the two outer bars, which show the lower and upper quartiles. In addition, there are extreme values (indicated by the dots above the boxplot) present for cell types C and D. We can also see that protein expression levels in some groups are much more variable than in others; the protein expression levels in group B are very consistent, but for groups C, D and E they are much more varied. 



We can make a similar assessment of the data using histograms (plotted separately for each group in our dataset). See Figure \ref{fig:hist1}. In particular we can use these histograms to make an assessment of normality and constant variance which are two of the assumptions behind ANOVA (Section \ref{anova-assumptions}). Note the difference in the range of the x-axis on the different plots. 

<<hist1, fig.lp="fig:",fig.cap="Histogram of the protein expression levels for five cell types">>=
par(mfrow=c(2,3))
cols <- brewer.pal(5,"Set1")
hist(proteinData$A,xlab="A",col=cols[1],main="")
hist(proteinData$B,xlab="B",col=cols[2],main="")
hist(proteinData$C,xlab="C",col=cols[3],main="")
hist(proteinData$D,xlab="D",col=cols[4],main="")
hist(proteinData$E,xlab="E",col=cols[5],main="")
@

\bioccomment{If you are more-familiar with programming, you could write this chunk of code with a for loop (or similar)}

In its current format, the data are unsuitable for analysis using one-way ANOVA as both the normality assumption and the constant variance assumption are violated. We can sometimes overcome this issue by transforming the data, and in this case we can use a log-transformation to normalise the data (Note: you do not need to do this step if the assumptions of normality and constant variance hold). This can be carried out within R using the \Rfunction{log} function.

<<>>=
proteinData.ln <- log(proteinData)
head(proteinData.ln)
@

\bioccomment{If you are not sure what the \Rfunction{log} is doing, don't forget that you can bring-up the help page: \Rcode{?log}}

Hopefully the transformation will result in data that are normally distributed enough to meet the normality assumption. To check this, create another set of histograms in the same way you did on the untransformed data (See Figure \ref{fig:hist2})

<<hist2, fig.lp="fig:",fig.cap="Histogram of the natural-log-transformed protein expression levels for five cell types">>=
par(mfrow=c(2,3))
hist(proteinData.ln$A,xlab="A",col=cols[1],main="")
hist(proteinData.ln$B,xlab="B",col=cols[2],main="")
hist(proteinData.ln$C,xlab="C",col=cols[3],main="")
hist(proteinData.ln$D,xlab="D",col=cols[4],main="")
hist(proteinData.ln$E,xlab="E",col=cols[5],main="")
@

\subsubsection{Fitting the model}

Figure \ref{fig:hist2} now shows that most of the cell types are approximately normally distributed, though cell type A is still a little questionable. We'll proceed for now, as the one-way ANOVA is quite robust to small deviations from normality, but we must bear this in mind when interpreting out results. We can also see from Figure \ref{fig:hist2} that the variance in each of the five groups is now much more similar. They do NOT need to be perfectly the same, and in this case they are similar enough for the assumption of equal variance to be reasonable. Now that we are happy that the assumptions are reasonable, we can perform the one-way ANOVA. 

When perfoming linear regression or ANOVA in R, it is more convenient to transform our data into \textbf{long} format, which can be done using the \Rcode{gather} function from the package \CRANpkg{tidyr}. For comprehensive descriptions of data manipulation and tidy data, see Hadley Wickham's paper \cite{tidyr:paper} or video \cite{tidyr:video} on the subject. 

Once we have transformed our data, the \Rfunction{aov} function fits the analysis of variance model to our data. Diagnostic plots of the model fit can be visualised using the \Rfunction{plot} function (Figure \ref{fig:anova-fit}). Of the most interest is the QQ-plot, which allows us to assess whether the distribution of the response variable is sufficiently normally distributed for each group being compared.

<<anova-fit, fig.lp="fig:",fig.cap="Visualising the results of the ANOVA model">>=
library(tidyr)
anovaData <- gather(proteinData.ln)
head(anovaData)
mod <- aov(value~ key, data=anovaData)
mod
par(mfrow=c(2,2))
plot(mod)
@

The \Rfunction{summary} function allows us to assess the significance of the model:-
<<>>=
summary(aov(mod))

@


One of the assumptions of the one-way ANOVA, which we have already explored with our scatter plot, is that the variances in each of the five groups are approximately equal. We can formally test this assumption when carrying out a one-way ANOVA, using a Bartlett's test (although its use is cautioned due to multiple testing issues). The results provide a $p$-value indicating whether equal variance can be assumed, with a value less than 0.05 suggesting that equal variance should not be assumed. Note that if log-tranformed data are being used for ANOVA, the Bartlett's test should also be performed on the logged data. 

<<>>=
bt <- bartlett.test(value~key,data=anovaData)
bt
@



In this example, the $p$-value was \Sexpr{round(bt$p.value,3)}, so there is no evidence to suggest that the variances in each of the five groups aren't approximately equal to each other. If the Bartlett's test gives a significant $p$-value, we cannot assume equal variances across the groups and a non-parametric test, such as the Kruskal-Wallis test, should be used instead of a one-way ANOVA.

The equal variance assumption (on the log-tranformed data) is reasonable in this example, so we can go ahead and use the one-way ANOVA to analyse the data. The results of the one-way ANOVA provide a $p$-value of $<0.0001$ which is statistically significant. This suggests that there is evidence of a difference in the mean log-transformed (or geometric mean of the original) protein expression levels between two or more of the five cell types. As the result of the one-way ANOVA was significant, we may be interested in making further comparisons between pairs of groups, and we can do this with the post-hoc test results. Note: if the one-way ANOVA result had not been significant we would usually stop here and not look at the post-hoc test results.

<<>>=
post.tests<- TukeyHSD(mod)
post.tests
@


The post-hoc tests are actually just unpaired t-tests, but the results reported are adjusted for multiple testing. In this example, we want to compare all pairs of groups, but sometimes there may be specific groups that you wish to compare. You should plan your comparisons before starting your analysis and it is better, at least from a statistical viewpoint, to perform the least number of comparisons that will sufficiently answer your question(s). As we want to perform more than one t-test on this data (in fact we are performing 10 pairwise comparisons!), we must be careful to adjust for multiple testing. Here we see that there is a significant difference between cell types A v D (p value: \Sexpr{post.tests$key["D-A","p adj"]}), a very significant difference between cell types B v E (p value: \Sexpr{post.tests$key["E-B","p adj"]}) and C v D (p value: \Sexpr{post.tests$key["D-C","p adj"]}), and an extremely significant difference between cell types B v D (p value: \Sexpr{post.tests$key["D-B","p adj"]}). 

Just because a result is statistically significant does not mean that it is biologically or clinically important. You can refer to the mean difference column to judge whether a difference of the size seen in the data is likely to be of biological or clinical importance, but remember that these values are now on the natural log (ln) scale! You can transform back to the original scale by taking the exponential of the mean difference: in this example, the mean difference between B and D is \Sexpr{round(post.tests$key["D-B","diff"],3)} on the natural log scale. On the original scale this translates to $e^{\Sexpr{round(post.tests$key["D-B","diff"],3)}}  = \Sexpr{round(exp(1)^(round(post.tests$key["B-A","diff"],3)),3)}$, this is now the difference in geometric rather than the usual arithmetic means.

\subsection{The Kruskal-Wallis test}
\label{kruskalWallis}

This tests if $k$ independent samples are drawn from the same population.  As the Kruskal-Wallis test ranks the values, it is more powerful than the Median test (\ref{medianTest}), which only looks at direction of differences.  The Kruskal-Wallis test is derived from the one-way ANOVA (\ref{oneWayAnova}), but uses ranks rather than actual observations.  It is also the extension of the Mann-Whitney U test to greater than two groups. However, it will provide the same result as the Mann-Whitney U test if carried out on two groups. 

\subsubsection{Assumptions}

The assumptions of the Kruskal-Wallis test are:

\begin{itemize}
\item{1. The data have been collected from a randomly selected set of observations.}
\item{2. The dependent variable is at least at the ordinal level of measurement, i.e. the data are able to be ranked.}
\item{3. There are more than two independent groups.}
\item{4. There is independence of observations within each group and between the groups.  There are no repeated measures or multiple response categories.}
\item{5. The shapes of the distributions of the groups are similar.}
\end{itemize}

\subsubsection{Null Hypothesis}

If the last assumption holds then the hypotheses are:

$H_0$: The medians in the $k$ groups are equal.

$H_A$: There is a difference in medians between the $k$ groups.

If the last assumption does not hold:

$H_0$: The $k$ groups have the same shape and location.

$H_A$: The $k$ groups have a different shape and location.

The alternative hypothesis can be directional or non-directional.  If a significant result is obtained then post-hoc testing can be used to see where any differences lie. 

\subsubsection{Method}

If the assumptions are met the test can be used in the following way:

\begin{itemize}
\item{1. Determine the null and alternative hypothesis and $\alpha$ the level of significance for the test.}
\item{2. Rank all observations from lowest to highest.}
\item{3. Calculate the sum of the ranks for each group.}
\item{4. Calculate the average rank in each group, $R_i$, and the overall average rank, $R$ .}
\item{5. Calculate the test statistic $H$,
\begin{equation}
H = 12\frac{\sum n_i (R_i - R)^2}{N (N + 1)}
\end{equation}
where $n_i$ = the number of observations in group $i$,
$N$ = the total sample size}
\item{6. Compare this value with the $\chi^2$ distribution with $k-1$ degrees of freedom.  If the statistic is bigger than the critical value in the chi-square table, the result is significant.  If the result is significant, then pairwise post-hoc tests can be carried out.}
\end{itemize}

\subsubsection{Example}

The data are the reduction in weekly headache activity for three treatment groups, expressed as a percentage of the baseline data (example from Altman \cite{altman}). Note that we are assuming that the groups have a similar distribution as we are testing for a difference in medians. 

<<echo=FALSE,results='asis'>>=
headache <- matrix(c(62,74,86,74,91,37, 69,43,100,94,100,98, 50,-120,100,-288,4,-76), ncol=3)

colnames(headache) <- c("Relaxation / response feedback","Relaxation alone","Untreated")

print(xtable(headache),include.rownames = FALSE)
@


The null and alternative hypothesis for our test are as follows:-

$H_0$: The three samples come from populations with the same median.
$H_A$: At least one sample comes from a population with a different median.

Computing the ranks then gives the following table:


<<echo=FALSE,results='asis'>>=
N <- length(headache)
ranks <- matrix(rank(headache),ncol=3)
head.data <- data.frame("Relaxation/response feedback"=headache[,1], "Rank"=ranks[,1], "Relaxation alone"=headache[,2], "Rank" = ranks[,2], "Untreated" = headache[,3], "Rank"=ranks[,3])
head.data <- rbind(head.data, c(NA,colSums(ranks)[1], NA, colSums(ranks)[2],NA,colSums(ranks)[3]))
head.data <- rbind(head.data, c(NA,colSums(ranks)[1]/nrow(headache), NA, colSums(ranks)[2]/nrow(headache),NA,colSums(ranks)[3]/nrow(headache)))
head.data <- cbind(c(rep("", nrow(headache)),"Rank sum","(mean)"),head.data)
colnames(head.data) <- c("","Relaxation/response feedback","Rank","Relaxation alone","Rank","Untreated","Rank")
rownames(head.data) <- NULL
print(xtable(head.data),include.rownames = FALSE)

R <- (N+1)/2
R1 <- round(colSums(ranks)[1]/nrow(headache),3)
R2 <- round(colSums(ranks)[2]/nrow(headache),3)
R3 <- round(colSums(ranks)[3]/nrow(headache),3)
headache.df <- data.frame(headache)
headache.df <- gather(headache.df)
kt <- kruskal.test(value~key,data=headache.df)


@

We then have all the values that we need in order to compute the test statistic:

$R = \frac{N +1}{2} =  \frac{\Sexpr{N} + 1}{2} = \Sexpr{(N+1)/2}$ 

$R_1 = \Sexpr{R1}$; $R_2 = \Sexpr{R2}$; $R_3 = \Sexpr{R3}$

$H = \frac{12(\Sexpr{nrow(headache)}(\Sexpr{R1} - \Sexpr{R})^2 + \Sexpr{nrow(headache)}(\Sexpr{R2} - \Sexpr{R})^2 + \Sexpr{nrow(headache)}(\Sexpr{R3}-\Sexpr{R})^2)}{\Sexpr{N} (\Sexpr{N}+1)} = 5.69$

However, $\chi^2_{2,0.05}=5.99,5.69<5.99$. Therefore there is not sufficient evidence to reject the null hypothesis at the 5\% level of significance, $p=0.06$

\subsubsection{Analysis in R}

A quick boxplot of the raw data reveal that the assumptions for One-Way ANOVA (\ref{anova-assumptions}) are not satisfied and we need to take a non-parametric approach.

<<>>=
headache <- matrix(c(62,74,86,74,91,37, 
                     69,43,100,94,100,98, 
                     50,-120,100,-288,4,-76), ncol=3)

colnames(headache) <- c("Relaxation/response feedback","Relaxation alone","Untreated")
boxplot(headache)
@

To see how to compute the statistic in R, we first have to structure the data in a more convenient format using the \Rcode{gather} function from the \CRANpkg{tidyr} package (\ref{dataManip}). We can then use the \Rcode{kruskal.test} function, which applies the Kruskal-Wallis test.

<<>>=
library(tidyr)
headache <- data.frame(headache)
headache <- gather(headache)
kt <- kruskal.test(value~key,data=headache)
kt
names(kt)
kt$statistic
kt$p.value
@

\subsubsection{Presentation of Results}

The results of the Kruskal-Wallis test could be reported in the following way:

\begin{quote}
The results of the Kruskal-Wallis test indicate that there was no evidence of differences in the reduction in weekly headache activity for three treatment groups (χ2k-w=5.69, p=0.051).
\end{quote}

\subsubsection{Advantages and Limitations}

The Kruskal-Wallis test is very simple and easy to understand and use.  It does not require equal sample sizes between groups.  It is one of the most popular non-parametric tests.  It is also one of the most powerful (i.e. it is very likely to reject the null hypothesis given that it is false) non-parametric tests for continuous data.


\subsection{Friedman's test}

The Friedman test extends the previously mentioned Wilcoxon signed ranks test to more than two repeated values at more than two time points.  Alternatively, to more than two matched blocks where the individuals of each block are randomly assigned to a group.

The test examines the ranks at the different time points or in matched blocks and tests whether the continuous underlying distribution of the variables is the same.  It is the non-parametric equivalent of the repeated measures ANOVA.

\subsubsection{Null Hypothesis}

$H_0$: There is no difference in median between the groups being tested.

$H_A$: There is at least one difference in median between the groups.

This is a non-directional alternative hypothesis.  If the alternative hypothesis is to be directional a more powerful way of analysing the data would be to carry out planned comparisons, with the appropriate correction for multiple testing.  Alternatively, overall tests of significance followed by post-hoc tests can be used.

\subsubsection{Assumptions}

\begin{itemize}

\item{1. The data to be analysed are continuous and at least at the ordinal level of measurement.}
\item{2. The data from a randomly selected sample are either multiple observations from a single sample across more than two time periods or conditions.  Otherwise, the data are blocks of matched subjects in which the subjects from a given block are each randomly assigned to one of the three or more conditions.}
\item{3. The subjects or blocks of subjects are independent; that is, the results within one block do not have an influence on the results within the other blocks.}
\end{itemize}

\subsubsection{Method}


\begin{itemize}
\item{1. Construct the null and alternative hypotheses.}
\item{2. Construct a two-way table with N (the number of subjects or matched sets of subjects) rows and k (the number of conditions or data collection periods) columns.}
\item{3. Rank each person's scores from lowest to highest and sum ranks in each column.}
\item{4. If the null hypothesis is not true then the sum of the columns will vary from column to column.  The Friedman test examines the extent to which these column sums vary from what is expected using the following formula:
\begin{equation}
F_r = \frac{12}{Nk(k+1)}\sum_j R^2_j - 3N(k+1)
\end{equation}

where:
	$R_j$ = the sum of the ranks for column $j$
	$N$ = the number of subjects
	$k$ = the number of periods or conditions.
	}
\item{5. Look $F_r$ up in tables of Friedman’s distribution.}	
\item{6. Reject the null hypothesis in favour of the alternative hypothesis if the $F_r$ value is greater than (or equal to) the value in the tables.}
\end{itemize}
Note: If $N$ and $k$ are sufficiently large, then $F_r$ can be compared to a $\chi^2$ distribution on $k-1$ degrees of freedom.


\subsubsection{Example}
The data for this example is taken from Rubin and Peter's paper \cite{rubinPeters}.  The Friedman test will be used to study whether or not hydralazine would relieve high blood pressure in the lungs.


<<echo=FALSE,results='asis'>>=
rubinPeters <- data.frame(Person = 1:4, Before=c(22.2,17,14.1,17), After48Hours = c(5.4,6.3,8.5,10.7), After6Months = c(10.6,6.2,9.3,12.3))
print(xtable(rubinPeters,caption="Total pulmonary resistance before and after hydralazine"),include.rownames=FALSE)

@

<<echo=FALSE>>=
rubinPeters <- data.frame(Person = 1:4, 
                          Before=c(22.2,17,14.1,17), 
                          hrs48 = c(5.4,6.3,8.5,10.7), 
                          mnths6 = c(10.6,6.2,9.3,12.3))
@


<<echo=FALSE,results='asis'>>=
N <- length(rubinPeters[,2:4])
ranks <- t(apply(rubinPeters[,2:4],1,rank))


head.data <- data.frame("Person" = rubinPeters[,1], "Before" = rubinPeters[,2], "Rank"=ranks[,1], "hrs48"=rubinPeters$hrs48,"Rank"=ranks[,2], "mths6"=rubinPeters$mnths6,Rank=ranks[,3])


head.data <- rbind(head.data, c(NA,NA,colSums(ranks)[1], NA, colSums(ranks)[2],NA,colSums(ranks)[3]))
R1 <- colSums(ranks)[1]
R2 <- colSums(ranks)[2]
R3 <- colSums(ranks)[3]

N <- nrow(rubinPeters)
k <- ncol(rubinPeters[,2:4])

#head.data <- rbind(head.data, c(NA,colSums(ranks)[1]/nrow(headache), NA, colSums(ranks)[2]/nrow(headache),NA,colSums(ranks)[3]/nrow(headache)))
#head.data <- cbind(c(rep("", nrow(headache)),"Rank sum","(mean)"),head.data)
colnames(head.data) <- c("Person","Before","Rank","After48Hours","Rank","After6Months","Rank")
rownames(head.data) <- NULL
print(xtable(head.data),include.rownames = FALSE)
fr <-  friedman.test(as.matrix(rubinPeters[,-1]))
@

$F_r = \frac{12}{\Sexpr{N} \times \Sexpr{k}(\Sexpr{k} + 1)}[\Sexpr{R1}^2 + [\Sexpr{R2}^2 + \Sexpr{R3}^2] - [3 \times \Sexpr{N}(\Sexpr{k}+1)] = \Sexpr{fr$statistic}$

As \Sexpr{fr$statistic} is the same as the value in the Friedman's distribution table there is sufficient evidence to reject the null hypothesis and conclude that at least one group is different from the others.

\subsubsection{Analysis in R}
We create a data frame representation in R, and produce the boxplots:-

<<>>=
rubinPeters <- data.frame(Person = 1:4, 
                          Before=c(22.2,17,14.1,17), 
                          hrs48 = c(5.4,6.3,8.5,10.7), 
                          mnths6 = c(10.6,6.2,9.3,12.3))
boxplot(rubinPeters[,2:4])
beeswarm(rubinPeters[,2:4],add=TRUE)
@


<<>>=
fr <-  friedman.test(as.matrix(rubinPeters[,-1]))
fr
@

\subsubsection{Post-hoc testing}
If the Friedman test shows that there is a difference in medians between the groups it is possible to carry out post-hoc testing to see which groups there is actually a difference between.  This is done by comparing average ranks in all the pairs or comparing to baseline.  The null hypothesis that there is no difference in mean ranks between the pairs, will be rejected if the absolute value of these differences is greater than a specified critical value.  If the following condition holds, the null hypothesis will be rejected:

\begin{equation}
|\bar{R_i} - \bar{R_j}| \geq Z_\alpha/[k(k-1)] \sqrt{\frac{k(k+1)}{6N}}
\end{equation}

where;
$R_i$ = the mean rank in period or condition $i$,
$R_j$ = the mean rank in period or condtion $j$,
$Z_\alpha$ = the critical $z$ value for $\alpha'$,
$\alpha'$ = $\alpha/[k(k-1)]$,
$k$ = the number of periods or conditions,
$N$ = the number of subjects.

In the example above, the average ranks for the three time points are 3, 1.25 and 1.75.  Since $k$= 3 and $\alpha=0.05$ the critical value of the $z$-statistic is a $z$ for which $\alpha'$ = 0.05 / 3(2) = 0.0083.  Looking this value up in the tables for the Normal distribution gives $z$ = 2.39.  The critical value therefore, is: 

$2.39 \sqrt{\frac{3(4)}{6(4)}} = 1.68$

The absolute values of the three comparisons are:

$|\bar{R_1} - \bar{R_2}| = |3 - 1.25| = 1.75 > 1.68$

$|\bar{R_1} - \bar{R_3}| = |3 - 1.75| = 1.25 < 1.68$

$|\bar{R_2} - \bar{R_3}| = |1.25 - 1.75| = 0.5 < 1.68$


The comparison between before treatment and 48 hours after treatment is the only one that is greater than the critical value of 1.68.  Therefore, we can conclude that according to the post-hoc approach, hydralazine only relieves high blood pressure in the lungs 48 hours after the treatment.  This effect was not maintained 6 months after treatment.

It is also possible to use the Wilcoxon ranked sign tests for post-hoc testing.  The procedure is carried out in the same way as described before.  However, the Bonferroni correction must be applied to allow for multiple testing.  That is the critical value of $\alpha$ becomes $\alpha'=\alpha/k$ where $k$ is the number of tests to be carried out and $\alpha$ is the original significance level.  The value of $\alpha'$ is the one looked at in the table or that the output $p$-value is compared against.

\subsubsection{Presentation of the Results}

The results of the Friedman test could be reported in the following way:

\begin{quote}
The results of the Friedman test indicate that there is a significant difference in median total pulmonary resistance across the three time periods.  Therefore, we can conclude that hydralazine alters total pulmonary resistance (p=0.042).  

Post-hoc analyses with the adjustment of the two-tailed level to 0.0083 indicated that there were decreases in total pulmonary resistance from before treatment (Md=17.0) to 48 hours after treatment (Md= 7.4).  There was no evidence of other differences. 
\end{quote}

\textbf{Note:} That post-hoc testing was carried out here on a very small sample size as an illustration, in reality post-hoc testing would not be carried out on such a small sample size.

\subsubsection{Advantages and Limitations}

The Friedman test is very versatile and can be used with randomised block designs and multiple observations of a single sample.  It is useful when the dependent variable is not consistent with a normal distribution.

There are some drawbacks however, it is possible for the medians not to change and there still to be significant differences between groups.  Although it is often referred to as the Friedman two-way ANOVA by ranks, it is restricted to within group comparisons.  It is not possible to test between group comparisons.  This is a major disadvantage in clinical research as it is not possible to make experimental-control group comparisons.  Each group can be analysed separately and compare their results.  However, it is not possible to test a group and time interaction with independent groups.

\subsubsection{Summary}

Friedman's test, tests the null hypothesis that $k$ related groups come from the same population.  For each case, the $k$ observations are ranked from 1 to $k$; the test statistic is based on these ranks.  

After establishing a difference between one of the variables, post-hoc testing can be carried out to decide which of the groups are actually different.  An appropriate method for allowing for multiple testing must be carried out.

\subsection{Median Test}
\label{medianTest}
Ths Median test tests whether two or more independent samples are drawn from populations with the same median using the $\chi^2$ statistic.  It can be used when the assumptions of similarity of distributions for the Mann-Whitney U and Kruskal-Wallis (\ref{kruskalWallis}) tests are not met.

\subsubsection{Null Hypothesis}

$H_0$: There is no difference in medians amongst the groups being studied. 

$H_A$: There is at least one difference in medians amongst the groups being studied.

\subsubsection{Assumptions}
The assumptions of the Median test are:

\begin{itemize}
\item{1. The dependent variable is at least at the ordinal level of measurement.  The data are from two or more groups}
\item{2. The groups are independent and a subject can only be in one of the groups.}
\item{3. The assumptions of the $\chi^2$ test apply to the second half of the test (if these are not met then Fisher's exact test should be used.}
\end{itemize}

\subsubsection{Method}

If these assumptions are met then the test can be carried out in the following way:

\begin{itemize}
\item{1. Construct the null and alternative hypotheses and decide on $\alpha$ the level of significance for the test.}
\item{2. Treat the data as a single sample and calculate the overall median.}
\item{3. Separate the data into the various groups and classify the observations in each group as either above, below or equal to the overall median.  Calculate the number above and below or equal to the median, in each group.}
\item{4. Arrange these values into a $2\times c$ contingency table, where the two rows are: $\>$ or $\leq$ to the overall median.  The $c$ columns are the groups.}
\item{5. Calculate the $\chi^2$ statistic for the table, if the assumptions hold. }
\item{6. Compare the value of the chi-square statistic with the value in the tables on ($c-1$) degrees of freedom (where $c$ is the number of groups) at the pre-specified level of $\alpha$}
\item{7. Reject the null hypothesis of equal medians if $X^2$ exceeds the critical value of the $\chi^2$ distributions.}
\item{8. If the null hypothesis is rejected, it is then possible to do post-hoc testing on the individual groups to see which ones are significantly different.  This again will be using the median test, but applied to pairs of groups.}
\end{itemize}

\subsubsection{Example}
There are three groups with different types of dementia (data from Sanjana Nyatsanza, Fulbourn hospital).  Below are the patients’ scores on a mini mental state examination (MMSE).  The median test will be used to see if there is a significant difference between the groups.

<<echo=FALSE,results='asis'>>=
mmse <- data.frame(Group1=c(19,7,17,28,21,6,21,19,27,8,25), 
                   Group2 = c(16,22,30,24,22,23,22,28,29,29,0),
                   Group3 = c(4,9,30,29,25,22,25,26,27,18,10)
)
mmse.t <- t(mmse)
colnames(mmse.t) <- rep("",nrow(mmse))
xtable(mmse.t)
@

1. Stating the null hypothesis and significant level

$H_0$: There is no difference in medians between the groups. 

$H_A$: There is a difference in medians between the groups. 

$\alpha=0.05$

2. The overall median (i.e. the one in the middle when ranked in order) is \Sexpr{median(as.numeric(unlist(mmse)))}.

3. Classify the values in each group as above or below \Sexpr{median(as.numeric(unlist(mmse)))}.

<<echo=FALSE,results='asis'>>=
mmse.t2 <- data.frame(matrix(nrow = nrow(mmse.t)*2,ncol=ncol(mmse.t)))
mmse.t2[1,] <- mmse.t[1,]
mmse.t2[2,] <- ifelse(mmse.t[1,] > 22, ">22", "<=22")
mmse.t2[3,] <- mmse.t[2,]
mmse.t2[4,] <- ifelse(mmse.t[2,] > 22, ">22", "<=22")
mmse.t2[5,] <- mmse.t[3,]
mmse.t2[6,] <- ifelse(mmse.t[3,] > 22, ">22", "<=22")
print(xtable(mmse.t2),include.rownames = FALSE,include.colnames = FALSE)
@

<<echo=FALSE,results='asis'>>=
med <- median(as.numeric(unlist(mmse)))
class <- matrix(nrow=2,ncol=3)
class[1,] <- apply(mmse.t, 1, function(x) sum(x <= 22))
class[2,] <- apply(mmse.t, 1, function(x) sum(x > 22))
class <- cbind(class,rowSums(class))
class <- rbind(class, colSums(class))
colnames(class) <- c("Group1", "Group2","Group3","Total")
rownames(class) <- c("<=22", ">22","Total")
xtable(class)
@

5. Find the expected values for each of the cells:

<<echo=FALSE,results='asis'>>=
print(xtable(data.frame(X=c(6,5),Y=c(6,5),Z=c(6,5))),include.rownames = FALSE,include.colnames = FALSE)
@

and calculate the $\chi^2$ statistic

$\chi^2 = \sum_{ij} \frac{(O_{ij}-E_{ij})^2}{E_{ij}}$

$ = \frac{(8 -6)^2}{6} + \frac{(5 -6)^2}{6} + \frac{(5 -6)^2}{6} + \frac{(3 -5)^2}{5} + \frac{(6 -5)^2}{5} + \frac{(6 -5)^2}{5}$

$ = 0.667 + 0.167 + 0.167 + 0.8 + 0.2 + 0.2 = 2.20 $

6. The $\chi^2$ statistic for $\alpha=0.05$ is $5.99$

7. $\chi^2=2.20 < 5.99$, there is insufficient evidence to reject the null hypothesis that the medians are the same for all three groups, $p=0.33$

8. As this result is not significant, post-hoc testing could not be carried out as there are no significant differences between the groups.

The Median test is very straightforward and easy to apply and is particularly useful when the exact values of the scores (especially those at the extremes) are unknown.  The test only considers two states for the scores, above and below (or equal to) the median and does not take the size of the differences into account.  Therefore, the Median test is less powerful than the Mann-Whitney U and Kruskal-Wallis tests.

\subsubsection{Analysis in R}

The Median test is implemented in the \CRANpkg{RVAideMemoire} package (\ref{installPackages}). As usual we need to 'tidy' the input data frame (\ref{dataManip}) in order to define a factor for the test. The \Rfunction{mood.medtest} function implements the test and has the option to compute an exact, or approximate $p$-value.
<<>>=
library(RVAideMemoire)

mmse <- data.frame(Group1=c(19,7,17,28,21,6,21,19,27,8,25), 
                   Group2 = c(16,22,30,24,22,23,22,28,29,29,0),
                   Group3 = c(4,9,30,29,25,22,25,26,27,18,10)
)

mmse <- gather(mmse)

mood.medtest(value~key,data=mmse,exact=FALSE)

mood.medtest(value~key,data=mmse)
@


\subsection{Jonckheere-Terpstra Test}

Also known as the \textit{test for ordered alternatives} or a \textit{non-parametric test for trend}.  The Jonckheere-Terpstra test is used when the assumption that the independent variable is nominal in the Kruskal-Wallis test (\ref{kruskalWallis}) is violated i.e. the groups have an explicit order.  Since it allows the independent variable (the groups) to have an order it is more powerful than the Kruskal-Wallis test (\ref{kruskalWallis}) when the groups are ordered.  

\subsubsection{Null Hypothesis}

$H_0$: There is no difference in median values between the groups.  

$H_A$: The median values of the groups increase in a specific predetermined sequence.  

\subsubsection{Assumptions}

\begin{itemize}
\item{1. The data have been collected from a randomly selected set of observations.}
\item{2. The data to be analysed are continuous and at least at the ordinal level of measurement.}
\item{3. The $k$ groups must be ordinal with a predetermined order.}
\item{4. Under the null hypothesis it is assumed that each sample is from the same population.}
\end{itemize}

\subsubsection{Method}

\begin{itemize}
\item{1. Construct the null and alternative hypotheses and determine the level of significance, $\alpha$.}
\item{2. Specify the order of the groups, which need not be equal sized.}
\item{3. Cast the data into a two-way table with the groups in the pre-specified order, arranged from smallest to largest.}
\item{4. Within each group order the data from smallest to largest. }
\item{5. Count the total number of times each value in the first group precedes (is lower than) a value in the subsequent groups this is the precedent count for the group.}
\item{6. Add 0.5 to each precedent count when a tie (equal value) occurs between groups.}
\item{7. Find the precedent count for the remaining groups and sum over the groups to give $J$, the test statistic.}
\item{8. Compare this value to that in the tables for $J$.  If the statistic is greater than or equal to the critical value in the Jonckheere-Terpstra test tables, the result is significant.  If the result is significant, the null hypothesis is rejected in favour of the alternative hypothesis.}
\end{itemize}

\subsubsection{Large Sample Size}

1. When the sample size is large the distribution of $J$ tends to a normal distribution, with mean:-

\begin{equation}
\mu_j = \frac{N^2 - \sum_{j=1}^kn^2_j}{4}
\end{equation}

and standard deviation

\begin{equation}
\sigma_j = \sqrt{\frac{N^2(2N +3) - \sum_{j=1}^k n_j^2 (2n_j+3)}{72}}
\end{equation}
where,

$N$ = total sample size,

$n_j$ = sample size of group $j$,

$k$ = number of groups,

$\sum$ = sum across groups,

A $z$ statistic can then be calculated as follows:

\begin{equation}
z = \frac{J-\mu_j}{\sigma_j}
\end{equation}

This $z$ statistic can then be compared to the tables for the normal distribution:


\subsubsection{Example}

Mcm-2 values were collected in a breast cancer study.  The median Mcm-2 value was expected to increase with histological grade (data below).  This hypothesis was tested using the Jonckheere-Terpstra test.

<<echo=FALSE,results='asis'>>=
grade <- data.frame(Grade1 = c(1.99,3.01,4.17,7.13,9.82,9.91,NA,NA), 
                    Grade2 = c(4.40,9.82,10.23,11.99,11.99,13.17,13.20,NA),
                    Grade3 =c(6.94,8.04,9.82,15.75,18.30,25.01,26.40,28.17))
print(xtable(grade),include.rownames=FALSE)
@

Stating assumptions and significance level

1. $H_0$: The median Mcm-2 value is the same across histological grades.

$H_A$: There is an increase is median Mcm-2 value as histological grade increases.
$\alpha=0.05$

2. The order of the groups is that of increasing histological grade.

5. The precedent counts for each pair of groups is in the table below.  

6. 0.5 has been added to each precedent count with a tied value between groups.

<<echo=FALSE,results='asis'>>=
pcounts <- data.frame(Grade1And2 = c(7,7,7,6,5.5,5,NA,"Total:37.5"),Grade1And3 = c(8,8,8,7,5.5,5,NA,41.5),Grade2And3 = c(8,5.5,5,5,5,5,5,38.5))
print(xtable(pcounts),include.rownames = FALSE)
@

7. $J = 37.5 + 41.5 + 38.5 = 117.5$

8. From the tables ($n_1=6, n_2=7,n_3=8,\alpha=0.05$) the critical value is 99. As $117.5 > 99$, there is sufficient evidence to reject the null hypothesis and conclude that there is a significant increase in median Mcm-2 value as histological grade increases.

\subsubsection{Presentation of results}

The results of the Jonckheere-Terpstra test could be reported in the following way:

\begin{quote}
The results of the Jonckheere-Terpstra test show that there is a trend for an increase in median Mcm-2 value as histological grade increases ($J$=117.5, p=0.003).
\end{quote}

\subsubsection{Analysis in R}

Performing the Jonckheere-Terpstra test in R requires the \CRANpkg{clinfun} package to be installed (\ref{installPackages}) (we recommend using this version of the test rather than versions in other packages).

<<>>=
library(clinfun)
grade <- data.frame(Grade1 = c(1.99,3.01,4.17,7.13,9.82,9.91,NA,NA), 
                    Grade2 = c(4.40,9.82,10.23,11.99,11.99,13.17,13.20,NA),
                    Grade3 =c(6.94,8.04,9.82,15.75,18.30,25.01,26.40,28.17))
grade <- gather(grade)
grade$key <- as.numeric(gsub("Grade","",grade$key))
jonckheere.test(grade$value,grade$key)

@


\subsubsection{Advantages and limitations}

The main advantage of the Jonckheere-Terpstra test is that unlike the Kruskal-Wallis test it allows the groups to have an order therefore it is more powerful than the Kruskal-Wallis test if the groups have a pre-specified order.  Since the Jonckheere-Terpstra test is a test for trend there is no need for post-hoc tests to see where differences lie after a significant result. 

The main limitation of the test is that the groups must have a pre-specified or explicit order.  It is not possible to look for an order and then test for a trend.  If there is no explicit order then a Kruskal-Wallis test should be used instead. 

\subsubsection{Summary}
The Jonckheere-Terpstra test is a more powerful alternative to the Kruskal-Wallis test when there is an explicit order to the groups.  It is a test for trend of increasing medians between the groups.  It is a much under-used nonparametric test; often the Kruskal-Wallis test is used where it would have been more appropriate to use the Jonckheere-Terpstra test.


\section{Regression}

\textbf{Regression Analysis} refers to a set of statistical techniques for modeling the relationship between two or more variables. One of these variables is the \textbf{response} variable (or \textbf{dependent} variable), while the other variables are known as \textbf{explanatory} (or \textbf{independent}) variables. Both response and explanatory variables are continuous, i.e. real numbers with decimal places, for example weights, intensities or growth rates.

Regression analysis is widely used for prediction, where the explanatory variables are known as \textbf{predictors} and the response variable is the thing that is being predicted.

One way of working out whether regression is the appropriate analysis for your data is to consider the most natural way of plotting the data in order to address the question you are asking. An XY scatter plot would point to regression, whereas analysis of variance (ANOVA) might be more appropriate if a better representation of the data was in the form of a boxplot.

<<scatterplot, fig.lp="fig:", fig.cap="Growth rate of E. coli at various concentrations of lactoferrin.", fig.width=5, fig.height=5, fig.align='center', echo=FALSE>>=
options(digits=3)
data <- read.csv("lactoferrin.csv")
plot(data, pch=16, xlab="Concentration", ylab="Growth rate")
@

An example of a dataset where regression analysis might be applied is shown in Figure \ref{fig:scatterplot}. This shows the results of a dose-response experiment where an \textit{E. coli} strain was exposed to various concentrations of the growth inhibitor, lactoferrin. In this example, we are interested in how the growth rate varies with concentration and a regression analysis would be suitable. Regression analysis involves fitting a model to the data that attempts to describe the relationship between the response and explanatory variables, in this case the growth rate and concentration.

There are several types of regression analysis \textemdash\ which you use will depend on the number of explanatory variables and the type of model to be fitted.

\begin{itemize}
\item{\textbf{Simple linear regression} \textendash\ the simplest and most frequently used, where there is one response variable and one explanatory variable and the relationship can be described through a linear model}
\item{\textbf{Multiple linear regression} \textendash\ fits a linear model using multiple explanatory variables}
\item{\textbf{Polynomial regression} \textendash\ used to test for non-linearity in a relationship}
\item{\textbf{Non-linear regression} \textendash\ to fit a specified non-linear model to the data}
\item{\textbf{Non-parametric regression} \textendash\ used when there is no obvious functional form}
\end{itemize}

\subsection{Linear Regression}

Linear regression involves fitting the simplest model of all, a linear model of the form:

\begin{equation}
y = ax + b
\end{equation}

where $y$ is the response variable and $x$ is a continuous explanatory variable. This should look familiar as the equation for a straight line graph. There are two parameters, $a$ and $b$. $a$ is the intercept, the value of $y$ when $x = 0$, and $b$ is the slope, or gradient, and is equal to the change in $y$ divided by the change in $x$ that brought about the change in $y$.

\subsubsection{The lactoferrin dataset}

Let's go back to our example dataset in Figure \ref{fig:scatterplot}, showing the effect of lactoferrin concentration on the growth rate of \textit{E. coli}. We can read the data into R using the \Rfunction{read.csv} function:

<<>>=
data <- read.csv("lactoferrin.csv")
data
@

We can generate something similar to Figure \ref{fig:scatterplot} using the \Rfunction{plot} function:

<<eval=FALSE>>=
plot(data$conc, data$growth, pch=16, xlab="Concentration", ylab="Growth rate")
@

Figure \ref{fig:scatterplot} shows that there is a roughly linear relationship between growth rate and lactoferrin concentration, with the growth rate decreasing with higher concentrations. We could estimate the parameters of a simple linear model by drawing a line through the data by eye and then calculating it's gradient and seeing where the line passes through the $y$-axis, i.e. the growth rate when the lactoferrin concentration is zero ($x = 0$). The R function \Rfunction{lm} will do this for us using a mathematical technique known as \textit{'least squares'}. Figure \ref{fig:bestfit} shows the resulting line of best fit. Also shown are the \textbf{residuals}, the vertical distances between the actual data points and the line of best fit, i.e. between the observed and fitted values.

<<bestfit, fig.lp="fig:", fig.cap="Line of best fit and residuals.", fig.width=5, fig.height=5, fig.align='center', echo=FALSE>>=
with(data, plot(conc, growth, pch=16, xlab="Concentration", ylab="Growth rate"))
model <- lm(growth ~ conc,data=data)
abline(model)
fitted <- fitted(model)
for (i in 1:10) lines(c(data$conc[i], data$conc[i]), c(data$growth[i], fitted[i]))
@

\subsubsection{Fitting the linear model}

The aim is to minimize the sum of squares of the residuals (also known as the error sum of squares, \textbf{SSE}), i.e. to find the minimum of

\begin{equation}
SSE = \sum_i(y_i - a - bx_i)^2
\end{equation}

More formally, the regression model is written as:

\begin{equation}
y_i = a + bx_i + \varepsilon_i
\end{equation}

where $\varepsilon_i$ is the error term and is assumed to be normally distributed:

\begin{equation}
\varepsilon_i \sim \mathcal{N}(0, \sigma^2)
\end{equation}

Now we can write the error sum of squares as:

\begin{equation}
SSE = \sum_i\varepsilon_i^2 = \sum_i(y_i - a - bx_i)^2
\end{equation}

Mathematically, we set the derivative of this function with respect to the slope to zero ($dSSE/db = 0$), do the same for the derivative with respect to the intercept ($dSSE/da = 0$), and then solve the resulting simultaneous equations. This is left as an exercise for the more mathematically inclined, or see "Statistics: An Introduction using R" by Michael Crawley \cite{crawley:statsr}.

In R, the tilde symbol, '$\sim$', is used in describing a model and can be read as 'is modelled as a function of'. In our example, the model would be written:

\begin{equation}
growth \sim conc
\end{equation}

The response variable goes on the left of the tilde and the explanatory variable on the right hand side. Our model can be read 'growth rate is modelled as a function of concentration'.

To obtain the parameters of the model, also known as the coefficients, we fit a linear model using the \Rfunction{lm} function:

<<>>=
model <- lm(growth ~ conc, data=data)
model
@

We can show the line of best fit on the plot using the \Rfunction{abline} function:

<<eval=FALSE>>=
plot(data, pch=16, xlab="Concentration", ylab="Growth rate")
abline(model)
@

and the \Rfunction{fitted} function will give us the fitted values for each data point:

<<>>=
fitted <- fitted(model)
fitted
@

The residuals are the differences between the observed values and the fitted values:

<<>>=
residuals <- data$growth - fitted
residuals
@

Coefficient, residuals and other properties of the model can be obtained using a number of useful functions or accessed directly through the model object:

<<>>=
coefficients(model)
residuals(model)
names(model)
model$coefficients
model$residuals
@

A noteable property of linear regression is that the line of best fit passes through the point $(\bar{x}, \bar{y})$ where $\bar{x}$ and $\bar{y}$ are the mean values of x and y respectively. 

<<>>=
mean(data$conc)
mean(data$growth)
model$coefficients[1] + model$coefficients[2] * mean(data$conc)
@

\subsubsection{Calculating standard errors in the regression parameters}

We would like to know how reliable are our estimates for the regression parameters, i.e. the slope and the intercept. For this we consider the total variation in $y$, represented by the total sum of squares of $y$:

\begin{equation}
SSY = \sum_i(y_i - \bar{y})^2
\end{equation}

The fitted values are represented formally by the symbol $\hat{y}_i$ and the sum of squares of the residuals, SSE, can be written as follows:

\begin{equation}
\hat{y}_i = a + bx_i
\end{equation}

\begin{equation}
SSE = \sum_i(y_i - \hat{y}_i)^2
\end{equation}

Computing the SSE in R is straightforward:

<<>>=
SSE <- sum(residuals^2)
SSE
@

The total variation in $y$, $SSY$, can be partitioned into separate components for the variation that is explained by the model, denoted by $SSR$, and the unexplained variation that is the error sum of squares, $SSE$, we've already calculated.

\begin{equation}
SSY = SSR + SSE
\end{equation}

The variation that is explained by the model is called the regression sum of squares, denoted by $SSR$, is given by:

\begin{equation}
SSR = \sum_i(\hat{y}_i - \bar{y}_i)^2
\end{equation}

Although the \Rfunction{lm} function does all the hard work for you, the following shows how to compute the variances in R from our data:

<<echo=FALSE>>=
options(digits=4)
@

<<>>=
mean_growth <- mean(data$growth)
mean_growth
SSY <- sum((data$growth - mean_growth)^2)
SSY
SSR <- sum((fitted - mean_growth)^2)
SSR
SSE <- SSY - SSR
SSE
@

<<echo=FALSE>>=
error_variance <- SSE / (length(data$growth) - 2)
F_ratio <- SSR / error_variance
@

These sources of variation are laid out in an analysis of variance table in Table \ref{regression-anova-table}. The mean squares column contains values for the variance for each source, calculated as:

\begin{equation}
\mathrm{variance} = \frac{\mathrm{sum\ of\ squares}}{\mathrm{degrees\ of\ freedom}}
\end{equation}

<<echo=FALSE>>=
options(digits=3)
@

\begin{table}[t]
\centering
\begin{tabular}{| l | c | c | c | c |}
\hline
\textbf{Source} & \textbf{Sum of squares} & \textbf{Degrees of freedom} & \textbf{Mean Squares} & \textbf{$F$ ratio} \\
\hline
Regression & $SSR = \Sexpr{SSR}$ & 1 & \Sexpr{SSR} & \Sexpr{F_ratio} \\
\hline
Error & $SSE = \Sexpr{SSE}$ & \Sexpr{length(data$growth) - 2} & $s^2 = \Sexpr{error_variance}$ & \\
\hline
Total & $SSY = \Sexpr{SSY}$ & \Sexpr{length(data$growth) - 1} & & \\
\hline
\end{tabular}
\caption{ANOVA table for the lactoferrin dataset}
\label{regression-anova-table}
\end{table}

<<echo=FALSE>>=
options(digits=4)
@

The number of degrees of freedom is determined by considering the number of parameters estimated from the data for each sum of squares. For the total sum of squares, $SSY = \sum(y - \bar{y})^2$, there is just one parameter estimated from the data: the mean value, $\bar{y}$. So we have $n - 1$ degrees of freedom, where $n$ is the number of observations (\Sexpr{length(data$growth)} in this case). Similarly, in order to calculate the error sum of squares, $SSE = \sum(y - \hat{y})^2 = \sum(y - a - bx)^2$, we need to know the values of two parameters, $a$ and $b$. These are estimated from the data, so the number of degrees of freedom are $n - 2$.

The number of degrees of freedom for the regression is more difficult to understand but if we consider that the regression degrees of freedom and the error degrees of freedom should add up to the total number of degrees of freedom, then we can see that this value must be 1. In the regression model, the fitted line is specified by two parameters, the slope and the intercept. But the fitted line must go through the mean of the response variable. This fixes the intercept and leaves just one degree of freedom for estimating the slope of the line.

The value of most interest in Table \ref{regression-anova-table} is the error variance, $s^2$.

\begin{equation}
s^2 = \frac{SSE}{n - 2}
\end{equation}

This is used in computing the standard errors in both the slope and intercept (see \cite{crawley:statsr} for the formulae).

The $F$ ratio is the ratio between the regression variance and the error variance.

<<>>=
error_variance <- SSE / (length(data$growth) - 2)
error_variance
F_ratio <- SSR / error_variance
F_ratio
@

This can be used to test for a non-zero slope in the linear regression, where the null hypothesis is that the slope is zero, i.e. there is no dependence of the response variable on the explanatory variable. The ANOVA table and $p$-value for the test are computed in R by calling the \Rfunction{anova} function:

<<>>=
anova(model)
@

\subsubsection{Summarizing the linear model}

The \Rfunction{summary} function shows the estimated slope and intercept parameters and their standard errors and is the most useful function for summarizing the results of a regression analysis. In practice you will not calculate the values of $SSY$, $SSE$, etc., longhand as above, but instead just call the \Rfunction{summary} function.

<<>>=
summary(model)
@

The output includes the standard errors for both the concentration coefficient (slope) and intercept. A $t$-statistic is given for each coefficient, calculated by dividing the estimated value by the standard error for the coefficient. From this a $p$-value is also computed.

The coefficients, errors and various statistics can be accessed directly through the summary object returned by the \Rfunction{summary} function or using the \Rfunction{coefficients} function passing the summary object.

<<>>=
summary <- summary(model)
names(summary)
summary$fstatistic
coefficients(summary)
summary$coefficients
summary$coefficients[1,2]
@

Also given is the residual error; this is the square root of the error variance we calculated earlier.

<<>>=
summary$sigma
sqrt(error_variance)
@

\subsubsection{Confidence intervals for the model parameters}

Confidence intervals on the model coefficients are calculated from the $t$-value and the standard error. Recall that in Student's $t$-distribution, values of $t$ are the numbers of standard errors to be expected with specified probability and for a given number of degrees of freedom.

\begin{equation}
\mathrm{confidence\ interval} = t\mathrm{-value} \times \mathrm{standard\ error}
\end{equation}

\begin{equation}
CI_{95\%} = t_{(\alpha=0.025,\mathrm{d.f.}=8)} \times \mathrm{s.e.}
\end{equation}

In R, the \Rfunction{confint} function computes the confidence intervals for the parameters of a model:

<<>>=
confint(model, level = 0.95)
@

\subsubsection{Measuring the degree of fit}

The output from the \Rfunction{summary} method includes a value for $r^2$. This is a measure of the degree of fit and is calculated as the fraction of the total variation in the response variable, $y$, that is explained by the regression.

\begin{equation}
r^2 = \frac{SSR}{SSY}
\end{equation}

<<>>=
r_squared <- SSR / SSY
r_squared
@

This varies from 1 when the regression explains all of the variation in $y$ (all the observed points lie on the line of best fit, $SSR = SSY$ and $SSE = 0$) to 0 when the regression explains none of the variation ($SSE = SSY$, $SSR = 0$). The square root of this quantity, $r$, is the correlation coefficient (Pearson's product-moment correlation).

<<>>=
cor.test(data$growth, data$conc)
cor(data$growth, data$conc) ^ 2
@

\subsubsection{Checking the model assumptions}

Aside from the assumption that there is a linear relationship between the explanatory variable and the response variable, linear regression also makes the following assumptions:

\begin{itemize}
\item{\textbf{Constant variance (homoscedasticity)} \textendash\ the variance of the errors is constant across the range of values for the explanatory variable}
\item{\textbf{Independence of errors} \textendash\ the errors in the response variables are uncorrelated with each other}
\item{\textbf{Normality of errors} \textendash\ the residuals follow a normal distribution}
\end{itemize}

Diagnostic plots for the model can be visualized using the \Rfunction{plot} function (Figure \ref{fig:lmfitdiag}). The first graph shows the residuals plotted against the fitted values; ideally these should look random without any structure or pattern in the plot. It would be a problem if there was a clear trend of increasing scatter as the fitted values get larger, i.e. violating the assumption of constant variance. The second plot, the quantile-quantile plot (QQ plot), is also worth looking at as this can highlight problems with non-normality of errors. If the errors are normally distributed this plot should be a straight line. An S-shaped or banana-shaped plot would indicate a need to fit a different model or to transform the data.

<<lmfitdiag, fig.lp="fig:", fig.cap="Diagnostic plots for checking the linear model fitted to the lactoferrin data.", fig.width=10, fig.height=10, fig.align='center'>>=
par(mfrow=c(2,2))
plot(model)
@

Outliers may have a strong influence on the fitted slope and intercept. The residuals vs. leverage plot shows labeled points that represent cases that may need investigating as potentially having undue influence on the regression relationship (cases 2, 8 and 9 in our example dataset).

\subsubsection{Using the model for prediction}

One use of the regression model is to predict or estimate the value of the response variable for new values of the explanatory variable(s). For our example, we may wish to estimate the growth rate of \textit{E. coli} for some new concentrations of lactoferrin. We can use the \Rfunction{predict} function to do this:

<<>>=
concentrations <- data.frame(conc = c(2.5, 6.5, 12.0))
predicted_growth <- predict(model, concentrations)
predicted_growth
@


\subsection{Beyond Simple Linear Regression}

The above introduction to linear regression focuses solely on the simplest case of a linear relationship between a single explanatory variable and the response variable. The relationship often will turn out not to be a straight line in which case we may need to fit a non-linear function or we may be able to convert the relationship into a linear one by transforming the response variable, e.g. by using its logarithm. We may also have several independent, potentially explanatory, variables and may be interested in understanding the functional relationships between these and the dependent variable to see what is causing the variation.

\subsubsection{Polynomial regression}

One way of assessing whether we have a non-linear relationship is to use polynomial regression. This involves attempting to fit a polynomial function, e.g. a quadratic function of the form:

\begin{equation}
y = a + bx + cx^2
\end{equation}

In R the model would be written \Rcode{y \textasciitilde x + I(x\textasciicircum 2)} where the \Rfunction{I} function is needed to treat the \Rcode{x\textasciicircum 2} term 'as is' (note that the \Rcode{\textasciicircum} symbol has a different meaning in the R syntax for specifying a model).

<<decay, fig.lp="fig:", fig.cap="Radioactive decay data set.", fig.width=5, fig.height=5, fig.align='center', echo=FALSE>>=
data <- read.csv("decay.csv")
x <- data$x
y <- data$y
plot(x, y, pch=16, xlim=c(0, 33), ylim=c(0, 155))
model <- lm(y ~ x)
abline(model)
@

Consider the radioactive decay data set shown in Figure \ref{fig:decay}. The data appear to follow a curve more than a straight line. This is also evident from the residuals diagnostic plot (Figure \ref{fig:decaylineardiag}). The residuals for high and low values of $x$ are positive while the residuals for the intermediate values of $x$ are mostly negative.

<<decaylineardiag, fig.lp="fig:", fig.cap="Residuals for the linear model fitted to the radioactive decay data.", fig.width=5, fig.height=5, fig.align='center', echo=FALSE>>=
plot(model, which=c(1))
@

We can assess the non-linearity in the relationship between $x$ and $y$ by fitting a polynomial regression model. The simplest such model involves adding a quadratic term, $x^2$.

<<>>=
quadratic <- lm(y ~ x + I(x^2))
summary(quadratic)
@

Note that the $p$-value for the quadratic term is highly significant, providing evidence that the relationship is non-linear. Also note that even though the function we are fitting is non-linear, it is still a linear model. If we replace the $x^2$ term by $z$, the relationship would be written $y = a + bx + cz$. Linear models do not necessarily have to involve a straight-line relationship between the response variable and the explanatory variables.

We can also use the \Rfunction{anova} function to compare the linear and quadratic models.

<<>>=
linear <- lm(y ~ x)
anova(quadratic, linear)
@

The $p$-value for the difference in the two models is the same as the significance of the quadratic term in the quadratic model, again indicating the improved fit.

However, the quadratic function is probably not the best model to fit to the decay data. Figure \ref{fig:decayquadratic} shows the fit from the quadratic function, from which we can see that the fitted response values begin to increase beyond a value of about 30. This is not consistent with the notion that these are decay data.

<<decayquadratic, fig.lp="fig:", fig.cap="Fitting a quadratic model to the radioactive decay data set.", fig.width=5, fig.height=5, fig.align='center', echo=FALSE>>=
plot(x, y, pch=16, xlim=c(0, 39), ylim=c(0, 155))
xv <- seq(0, 40, 0.1)
yv <- predict(quadratic, data.frame(x = xv))
lines(xv, yv)
@

\subsubsection{Linearizing the model by transformation}

For our radioactive decay data set, an exponential function of the form $y = ae^{-bx}$ might be more appropriate. This is an example where we can linearize the model by applying a transformation, in this case taking logarithms of both sides of the equation:

\begin{equation}
y = ae^{-bx}
\end{equation}

\begin{equation}
\ln(y) = \ln(a) - bx
\end{equation}

If we make $\ln(y)$ the response variable instead of $y$, we can use \Rfunction{lm} to fit an exponential curve:

<<>>=
exponential <- lm(log(y) ~ x)
summary(exponential)
@

Figure \ref{fig:decayexponential} compares the fitted quadratic and exponential models.

<<decayexponential, fig.lp="fig:", fig.cap="Fitting quadratic (solid line) and exponential (dotted line) models to the radioactive decay data set.", fig.width=5, fig.height=5, fig.align='center', echo=FALSE>>=
plot(x, y, pch=16, xlim=c(0, 33), ylim=c(0, 155))
lines(xv, yv)
yv2 <- exp(predict(exponential, data.frame(x = xv)))
lines(xv, yv2, lty=2)
@

\subsubsection{Multiple regression}

Sometimes we may want to fit a model using more than one explanatory variable. Multiple linear regression involves fitting a linear model of the form:

\begin{equation}
y = a + bx_1 + cx_2 + dx_3 + ...
\end{equation}

where $x_1$, $x_2$, $x_3$, etc. are our explanatory variables. The R code for fitting this model is:

<<eval=FALSE>>=
model <- lm(y ~ x1 + x2 + x3)
@

The quadratic function we fitted above to radioactive decay data is an example of multiple linear regression.

TODO: example and pairs plot

TODO: caution about overfitting, multicollinearity

TODO: interaction terms, forward/backward selection


\subsubsection{Using nominal variables in a multiple regression}

TODO

\subsubsection{Non-linear models}

In some situations a particular mechanistic model will lend itself to the data. Where this takes the form of a non-linear equation that cannot be linearized by transformation of the response variable or the explanatory variable (or both), the \Rfunction{nls} function can be used in place of \Rfunction{lm}. Initial guesses for each of the parameters will have to be supplied. For example, fitting a non-linear function of the form, $y = a - be^{-cx}$, can be carried out as follows:

<<eval=FALSE>>=
library(nls)
model <- nls(y ~ a - b * exp(-c * x), start = list(a = 50, b = 100, c = 0.05))
@

This is beyond the scope of what is covered in this introduction to regression analysis; see \cite{crawley:statsr} for more details.


\bibliography{stats}


\end{document}
